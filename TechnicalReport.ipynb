{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some useful mysklearn package import statements and reloads\n",
    "import importlib\n",
    "\n",
    "import mysklearn.myutils\n",
    "import mysklearn.plot_utils\n",
    "importlib.reload(mysklearn.myutils)\n",
    "import mysklearn.myutils as myutils\n",
    "\n",
    "# uncomment once you paste your mypytable.py into mysklearn package\n",
    "import mysklearn.mypytable\n",
    "importlib.reload(mysklearn.mypytable)\n",
    "from mysklearn.mypytable import MyPyTable\n",
    "\n",
    "# uncomment once you paste your myclassifiers.py into mysklearn package\n",
    "import mysklearn.myclassifiers\n",
    "importlib.reload(mysklearn.myclassifiers)\n",
    "from mysklearn.myclassifiers import MyKNeighborsClassifier, MyDummyClassifier, MyNaiveBayesClassifier, MyDecisionTreeClassifier, MyRandomForestClassifier\n",
    "import mysklearn.myevaluation\n",
    "importlib.reload(mysklearn.myevaluation)\n",
    "import mysklearn.myevaluation as myevaluation\n",
    "\n",
    "# Initialize the all the Classifiers\n",
    "knn_classifier = MyKNeighborsClassifier()\n",
    "dummy_classifier = MyDummyClassifier()\n",
    "naive_class = MyNaiveBayesClassifier()\n",
    "tree_classifier = MyDecisionTreeClassifier()\n",
    "forest_classifier = MyRandomForestClassifier(n_trees=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = MyPyTable().csv_to_mypytable(\"input_file/fifa_players.csv\")\n",
    "\n",
    "rating_column = table.get_column(\"overall_rating\")\n",
    "indexes_to_drop = [index for index, row in enumerate(rating_column) if int(row) < 70]\n",
    "print(indexes_to_drop)\n",
    "print(\"size before: \" , len(table.data))\n",
    "table.drop_rows(indexes_to_drop)\n",
    "print(\"size after: \" , len(table.data))\n",
    "\n",
    "\n",
    "table.remove_column(\"name\")\n",
    "table.remove_column(\"full_name\")\n",
    "table.remove_column(\"birth_date\")\n",
    "table.remove_column(\"age\")\n",
    "table.remove_column(\"weight_kgs\")\n",
    "table.remove_column(\"nationality\")\n",
    "table.remove_column(\"overall_rating\")\n",
    "table.remove_column(\"potential\")\n",
    "table.remove_column(\"value_euro\")\n",
    "table.remove_column(\"wage_euro\")\n",
    "table.remove_column(\"preferred_foot\")\n",
    "table.remove_column(\"international_reputation(1-5)\")\n",
    "table.remove_column(\"weak_foot(1-5)\")\n",
    "table.remove_column(\"body_type\")\n",
    "table.remove_column(\"release_clause_euro\")\n",
    "table.remove_column(\"national_team\")\n",
    "table.remove_column(\"national_rating\")\n",
    "table.remove_column(\"national_team_position\")\n",
    "table.remove_column(\"national_jersey_number\")\n",
    "table.remove_column(\"heading_accuracy\")\n",
    "table.remove_column(\"volleys\")\n",
    "table.remove_column(\"curve\")\n",
    "table.remove_column(\"sprint_speed\")\n",
    "table.remove_column(\"reactions\")\n",
    "table.remove_column(\"balance\")\n",
    "table.remove_column(\"jumping\")\n",
    "table.remove_column(\"strength\")\n",
    "table.remove_column(\"aggression\")\n",
    "table.remove_column(\"penalties\")\n",
    "table.remove_column(\"composure\")\n",
    "table.remove_column(\"sliding_tackle\")\n",
    "\n",
    "positions = table.get_column(\"positions\")\n",
    "discretized_positions = []\n",
    "for position in positions:\n",
    "    discretized_positions.append(myutils.classify_position(position))\n",
    "\n",
    "print(discretized_positions)\n",
    "table.pretty_print()\n",
    "print(\"size after: \" , len(table.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform Analysis (using kNN, Naive Bayes, Dummy, and Single Decision Tree Classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_class.header = [\n",
    "    \"height_cm\",\n",
    "    \"positions\",\n",
    "    \"skill_moves(1-5)\",\n",
    "    \"crossing\",\n",
    "    \"finishing\",\n",
    "    \"short_passing\",\n",
    "    \"dribbling\",\n",
    "    \"freekick_accuracy\",\n",
    "    \"long_passing\",\n",
    "    \"ball_control\",\n",
    "    \"acceleration\",\n",
    "    \"agility\",\n",
    "    \"shot_power\",\n",
    "    \"stamina\",\n",
    "    \"long_shots\",\n",
    "    \"interceptions\",\n",
    "    \"positioning\",\n",
    "    \"vision\",\n",
    "    \"marking\",\n",
    "    \"standing_tackle\"\n",
    "]\n",
    "# List of columns to include in the combined list\n",
    "columns_to_include = [\n",
    "    \"height_cm\",\n",
    "    \"positions\",\n",
    "    \"skill_moves(1-5)\",\n",
    "    \"crossing\",\n",
    "    \"finishing\",\n",
    "    \"short_passing\",\n",
    "    \"dribbling\",\n",
    "    \"freekick_accuracy\",\n",
    "    \"long_passing\",\n",
    "    \"ball_control\",\n",
    "    \"acceleration\",\n",
    "    \"agility\",\n",
    "    \"shot_power\",\n",
    "    \"stamina\",\n",
    "    \"long_shots\",\n",
    "    \"interceptions\",\n",
    "    \"positioning\",\n",
    "    \"vision\",\n",
    "    \"marking\",\n",
    "    \"standing_tackle\"\n",
    "]\n",
    "\n",
    "# Extract the indices of the specified columns\n",
    "column_indices = [table.column_names.index(col) for col in columns_to_include]\n",
    "# Construct the combined list directly from the table data\n",
    "combined_list = [[row[id] for id in column_indices] for row in table.data]\n",
    "# Extract the target\n",
    "target = discretized_positions\n",
    "\n",
    "#knn_accuracy, knn_error_rate = myutils.cross_val_predict( classifier=naive_class, X =combined_list, y=target, k=10, stratify=False )\n",
    "#print(knn_accuracy, knn_error_rate)\n",
    "myutils.perform_analysis(features = combined_list, targets = target, knn_classifier= knn_classifier, dummy_classifier= dummy_classifier, naive_class= naive_class, tree_classifier=tree_classifier)\n",
    "\n",
    "# evaluate using multiple different trees, F max number of features 5, and Number of Features being total dataset\n",
    "# forest_classifier = MyRandomForestClassifier(n_trees=10, max_features=5)\n",
    "# forest_classifier.fit(X=combined_list, y = target)\n",
    "# print(forest_classifier.tree_accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Tuning and Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample, seed\n",
    "import copy\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "seed(42)\n",
    "\n",
    "# List to store results for different combinations of N, M, and F\n",
    "results = []\n",
    "\n",
    "# Original combined list and target\n",
    "original_combined_list = copy.deepcopy(combined_list)\n",
    "original_target = copy.deepcopy(target)\n",
    "\n",
    "# Define different settings for N, M, and F\n",
    "N_values = [\n",
    "    len(original_combined_list) // 4,\n",
    "    len(original_combined_list) // 2,\n",
    "    len(original_combined_list),\n",
    "]  # Different dataset sizes (rows)\n",
    "M_values = [\n",
    "    len(columns_to_include) // 2,\n",
    "    len(columns_to_include),\n",
    "]  # Different number of features (columns)\n",
    "F_values = [3, 5, len(columns_to_include)]  # Different max features for Random Forest\n",
    "\n",
    "# Run experiments for each combination of N, M, and F\n",
    "for N in N_values:\n",
    "    for M in M_values:\n",
    "        for F in F_values:\n",
    "            accuracies = []\n",
    "            confusion_matrices = []\n",
    "\n",
    "            # Repeat each setting 5 times for better evaluation\n",
    "            for _ in range(5):\n",
    "                # Reduce rows (N): Sample N rows randomly from the original dataset\n",
    "                sampled_indices = sample(range(len(original_combined_list)), N)\n",
    "                sampled_data = [original_combined_list[idx] for idx in sampled_indices]\n",
    "                sampled_target = [original_target[idx] for idx in sampled_indices]\n",
    "\n",
    "                # Reduce features (M): Select the first M columns from the sampled data\n",
    "                reduced_data = [row[:M] for row in sampled_data]\n",
    "\n",
    "                # Initialize and fit the Random Forest Classifier\n",
    "                forest_classifier = MyRandomForestClassifier(n_trees=10, max_features=F)\n",
    "                forest_classifier.fit(X=reduced_data, y=sampled_target)\n",
    "\n",
    "                # Predict using the Random Forest and calculate accuracy\n",
    "                predictions = forest_classifier.predict(reduced_data)\n",
    "                accuracy = myevaluation.accuracy_score(sampled_target, predictions)\n",
    "                accuracies.append(accuracy)\n",
    "\n",
    "                # Compute confusion matrix\n",
    "                labels = list(set(sampled_target))  # Unique target classes\n",
    "                confusion_matrices.append(myevaluation.confusion_matrix(sampled_target, predictions, labels))\n",
    "\n",
    "            # Store the average accuracy and confusion matrices for the current combination\n",
    "            avg_accuracy = sum(accuracies) / len(accuracies)\n",
    "            results.append({\n",
    "                \"N\": N,\n",
    "                \"M\": M,\n",
    "                \"F\": F,\n",
    "                \"Accuracy\": avg_accuracy,\n",
    "                \"Confusion Matrices\": confusion_matrices,\n",
    "            })\n",
    "\n",
    "# Print the results for each setting of N, M, and F\n",
    "print(\"\\nTuning Results:\")\n",
    "for result in results:\n",
    "    print(f\"N: {result['N']}, M: {result['M']}, F: {result['F']}, Accuracy: {result['Accuracy']:.4f}\")\n",
    "    print(\"Confusion Matrices:\")\n",
    "    for matrix in result[\"Confusion Matrices\"]:\n",
    "        print(matrix)\n",
    "    print()\n",
    "\n",
    "best_result = max(results, key=lambda r: r[\"Accuracy\"])\n",
    "print(\"Best Configuration:\")\n",
    "print(f\"N: {best_result['N']}, M: {best_result['M']}, F: {best_result['F']}, Accuracy: {best_result['Accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Visualization Code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import mysklearn.plot_utils\n",
    "\n",
    "# Reload the plot_utils module\n",
    "importlib.reload(mysklearn.plot_utils)\n",
    "\n",
    "# Assuming `discretized_positions` is already computed\n",
    "# Compute the frequencies of the discretized positions\n",
    "unique_positions, counts = mysklearn.plot_utils.get_frequencies(discretized_positions)\n",
    "\n",
    "# Plot the histogram\n",
    "mysklearn.plot_utils.plot_histogram(\n",
    "    unique_positions,\n",
    "    counts,\n",
    "    title=\"Player Position Distribution\",\n",
    "    xlabel=\"Positions\",\n",
    "    ylabel=\"Count\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 1: Bar chart showing class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Ensure height and ball control are numeric\n",
    "height = table.get_column(\"height_cm\")\n",
    "ball_control = table.get_column(\"ball_control\")\n",
    "\n",
    "# Convert to numeric, handling non-numeric values\n",
    "def to_numeric(column, name):\n",
    "    try:\n",
    "        return [float(value) for value in column]\n",
    "    except ValueError:\n",
    "        print(f\"Non-numeric values found in '{name}' column. Replacing with NaN.\")\n",
    "        return [float(value) if value.isnumeric() else np.nan for value in column]\n",
    "\n",
    "height = to_numeric(height, \"height_cm\")\n",
    "ball_control = to_numeric(ball_control, \"ball_control\")\n",
    "\n",
    "# Drop rows with NaN\n",
    "valid_indices = [i for i in range(len(height)) if not np.isnan(height[i]) and not np.isnan(ball_control[i])]\n",
    "height = [height[i] for i in valid_indices]\n",
    "ball_control = [ball_control[i] for i in valid_indices]\n",
    "positions = [discretized_positions[i] for i in valid_indices]\n",
    "\n",
    "# Plot height vs. ball control by position\n",
    "plt.figure(figsize=(10, 6))  # Adjust figure size\n",
    "for position in set(positions):\n",
    "    indices = [i for i, pos in enumerate(positions) if pos == position]\n",
    "    plt.scatter(\n",
    "        [height[i] for i in indices],\n",
    "        [ball_control[i] for i in indices],\n",
    "        label=position,\n",
    "        alpha=0.6\n",
    "    )\n",
    "\n",
    "# Customize ticks and formatting\n",
    "plt.title(\"Height vs. Ball Control by Position\", fontsize=14)\n",
    "plt.xlabel(\"Height (cm)\", fontsize=12)\n",
    "plt.ylabel(\"Ball Control\", fontsize=12)\n",
    "plt.xticks(fontsize=10, rotation=45)  # Rotate x-axis labels for clarity\n",
    "plt.yticks(fontsize=10)\n",
    "plt.legend(title=\"Positions\", fontsize=10, title_fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"height_vs_ball_control.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 2: Scatter Plot Evaluating Height Vs Ball Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Ensure vision, short_passing, and crossing are numeric\n",
    "vision = table.get_column(\"vision\")\n",
    "short_passing = table.get_column(\"short_passing\")\n",
    "crossing = table.get_column(\"crossing\")\n",
    "\n",
    "# Convert to numeric, handling non-numeric values\n",
    "def to_numeric(column, name):\n",
    "    try:\n",
    "        return [float(value) for value in column]\n",
    "    except ValueError:\n",
    "        print(f\"Non-numeric values found in '{name}' column. Replacing with NaN.\")\n",
    "        return [float(value) if value.isnumeric() else np.nan for value in column]\n",
    "\n",
    "vision = to_numeric(vision, \"vision\")\n",
    "short_passing = to_numeric(short_passing, \"short_passing\")\n",
    "crossing = to_numeric(crossing, \"crossing\")\n",
    "\n",
    "# Drop rows with NaN\n",
    "valid_indices = [\n",
    "    i for i in range(len(vision))\n",
    "    if not np.isnan(vision[i]) and not np.isnan(short_passing[i]) and not np.isnan(crossing[i])\n",
    "]\n",
    "vision = [vision[i] for i in valid_indices]\n",
    "short_passing = [short_passing[i] for i in valid_indices]\n",
    "crossing = [crossing[i] for i in valid_indices]\n",
    "positions = [discretized_positions[i] for i in valid_indices]\n",
    "\n",
    "# Create a 3D scatter plot\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot each position group in a different color\n",
    "for position in set(positions):\n",
    "    indices = [i for i, pos in enumerate(positions) if pos == position]\n",
    "    ax.scatter(\n",
    "        [vision[i] for i in indices],\n",
    "        [short_passing[i] for i in indices],\n",
    "        [crossing[i] for i in indices],\n",
    "        label=position,\n",
    "        alpha=0.7\n",
    "    )\n",
    "\n",
    "# Customize plot\n",
    "ax.set_title(\"Vision vs Short Passing vs Crossing by Position\", fontsize=14)\n",
    "ax.set_xlabel(\"Vision\", fontsize=12)\n",
    "ax.set_ylabel(\"Short Passing\", fontsize=12)\n",
    "ax.set_zlabel(\"Crossing\", fontsize=12)\n",
    "ax.legend(title=\"Positions\", fontsize=10, title_fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 3: 3D plot of Vision, Short Passing, and Crossing  \n",
    "*(image gets cut off in docker container for some reason)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction:\n",
    "\n",
    "For this project, we utilized a dataset of FIFA players, which includes various attributes describing player characteristics, such as height, skill moves, passing accuracy, and positional data. The goal of the classification task was to predict the primary role of each player, discretized into four categories: Forward, Midfielder, Defender, and Goalkeeper. These roles were derived from the player’s positional attributes.\n",
    "\n",
    "The primary objective of the analysis was to evaluate the performance of different classifiers—including k-Nearest Neighbors (k-NN), Naive Bayes, Decision Trees, and Random Forests—on this classification task. Additionally, we explored the impact of varying key parameters in the Random Forest classifier, specifically the number of rows (N), number of features (M), and maximum features per split (F), to identify the optimal configuration for the dataset.\n",
    "\n",
    "Findings\n",
    "\n",
    "Through our experiments, the Random Forest classifier consistently outperformed other approaches, achieving the highest accuracy. The best results were obtained by tuning the parameters N, M, and F to balance dataset size, feature selection, and the complexity of tree splits. These results highlight the importance of parameter tuning in achieving high classification performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis:\n",
    "\n",
    "1. ##### Dataset Information\n",
    "The dataset consists of 5,667 instances and includes 20 attributes, of which 19 are integers and 1 is a float. The attribute being used as the label is \"position,\" which initially detailed specific roles such as Center Forward (CF), Right Wing (RW), Central Attacking Midfielder (CAM), Center Back (CB), Goalkeeper (GK), and others. To simplify the analysis and reduce complexity, these positions were discretized into four broader categories: Forward, Midfielder, Defender, and Goalkeeper. This transformation allowed for a more generalized and higher-level classification of player roles within the dataset.\n",
    "\n",
    "2. ##### Relevant Summary Statistics\n",
    "The dataset was evaluated using several classification models, yielding varying levels of performance. The **KNN Classifier** and the **Dummy Classifier** both achieved an accuracy of 0.35 with a corresponding error rate of 0.65, indicating that these models performed at baseline level. The **Naive Bayes Classifier** showed significant improvement, achieving an accuracy of 0.91 and an error rate of 0.09. The **Decision Tree Classifier** performed even better, with an accuracy of 0.95 and an error rate of 0.05. The best performance was observed with the **Random Forest Classifier**, which achieved an accuracy of 0.9710, further demonstrating the effectiveness of ensemble methods for this dataset.\n",
    "\n",
    "3. ##### Data Visualization  \n",
    "\n",
    "\n",
    "![Player Position Distribution](media/player_position_distribution.png)\n",
    "\n",
    "The bar chart above highlights the distribution of player positions within the dataset, revealing an uneven representation across roles. Midfielders and defenders dominate the dataset, each exceeding 1,750 players, while forwards are fewer, numbering approximately 1,250. Goalkeepers are the least represented, with fewer than 500 entries. This disparity could reflect the typical composition of football teams, where midfielders and defenders are more abundant compared to specialized roles like goalkeepers.\n",
    "\n",
    "Future data analysis could address this imbalance by employing techniques such as down-sampling the overrepresented positions (midfielders and defenders) to create a more balanced dataset. This approach would ensure that all positions are equally represented, enabling fairer comparisons and reducing potential biases in modeling or statistical analyses. For example, a balanced dataset would be particularly useful when analyzing positional attributes like performance metrics, injury rates, or salaries.  \n",
    "\n",
    "\n",
    "\n",
    "![Height vs. Ball Control by Position](media/height_vs_ball_control.png)\n",
    "\n",
    "The scatter plot above illustrates the relationship between height and ball control across different player positions, with points color-coded by role: midfielders (blue), defenders (orange), forwards (green), and goalkeepers (red).\n",
    "\n",
    "Key observations from the visualization include:\n",
    "- **Midfielders** (blue) generally exhibit high ball control regardless of their height, emphasizing their role in maintaining possession and playmaking.\n",
    "- **Forwards** (green) also demonstrate high ball control but with more variation across heights, reflecting the diverse physical profiles of attacking players.\n",
    "- **Defenders** (orange) display a wider range of ball control values, with taller defenders tending towards lower ball control scores, which may align with their focus on defensive rather than technical abilities.\n",
    "- **Goalkeepers** (red) stand out as the tallest group but with the lowest ball control values, underscoring their specialized role focused on shot-stopping rather than field play.\n",
    "\n",
    "This plot highlights how physical characteristics like height correlate with technical skills like ball control across positions. Further analysis could investigate outliers, such as taller players with exceptional ball control (like Cristiano Ronaldo) or shorter players excelling in defensive roles, to identify unique profiles. Additionally, these trends could be explored in the context of team strategies / team success.  \n",
    "\n",
    "\n",
    "\n",
    "![3D model of Vision v Short Passing v Crossing ability by position](media/3D_vision_short_passing_crossing.png)\n",
    "\n",
    "The 3D scatter plot above visualizes the interplay between vision, short passing, and crossing abilities across different player positions. Each point represents a player, color-coded by their position: midfielders (blue), defenders (orange), forwards (green), and goalkeepers (red). \n",
    "\n",
    "From the visualization, it is evident that:\n",
    "- **Midfielders** (blue) demonstrate high values across vision and short passing, with a strong representation in crossing abilities. This aligns with their role in creating plays and distributing the ball effectively.\n",
    "- **Defenders** (orange) are clustered primarily in regions with moderate short passing and crossing skills but exhibit lower vision, reflecting their less offensive roles.\n",
    "- **Forwards** (green) exhibit a mix of crossing and vision but slightly lower short passing capabilities compared to midfielders, as their focus is more on finishing plays rather than creating them.\n",
    "- **Goalkeepers** (red) are distinctively isolated, showing very low values across all three attributes, which corresponds to their primary focus on defensive responsibilities rather than passing or crossing.\n",
    "\n",
    "This visualization highlights clear positional trends in these attributes, offering insight into how player roles align with specific skills. Further analysis could involve examining how these distributions impact overall team performance or identifying outliers, such as goalkeepers with unexpectedly high passing attributes. Combining this data with match statistics could also reveal how these skill sets translate into practical contributions during games."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Results: \n",
    "\n",
    "#### Classification Techniques\n",
    "For this project, the task was to classify soccer players into one of four roles: **Forward**, **Midfielder**, **Defender**, or **Goalkeeper**. The classification techniques used were:\n",
    "\n",
    "1. **k-Nearest Neighbors (k-NN):**\n",
    "   - Predicts a player's role based on the roles of the nearest neighbors in the feature space.\n",
    "   - Uses Euclidean distance as the metric.\n",
    "2. **Naive Bayes Classifier:**\n",
    "   - A probabilistic classifier assuming independence between features.\n",
    "   - Suitable for handling categorical and numerical data.\n",
    "3. **Decision Tree Classifier:**\n",
    "   - A tree-based approach that partitions the data iteratively based on the attribute providing the highest information gain.\n",
    "4. **Random Forest Classifier:**\n",
    "   - An ensemble method combining multiple decision trees trained on random subsets of the data and features.\n",
    "   - Improves prediction stability and accuracy.\n",
    "5. **Dummy Classifier (Baseline):**\n",
    "   - Predicts the most frequent class in the dataset to provide a baseline for comparison.\n",
    "\n",
    "#### Implementation\n",
    "The classifiers were implemented using custom algorithms developed in Python. The dataset was preprocessed to:\n",
    "1. Filter players with an overall rating of 70 or above.\n",
    "2. Discretize player positions into four roles for classification.\n",
    "3. Standardize numerical attributes to improve model performance.\n",
    "\n",
    "Each classifier was tested using **cross-validation** with stratified splits to ensure representative class distributions in training and test sets. Random Forests were evaluated across different configurations of hyperparameters (`N`, `M`, and `F`):\n",
    "- **N:** Number of rows (instances) used during training.\n",
    "- **M:** Number of features (columns) used during training.\n",
    "- **F:** Maximum features considered for each split in the random forest.\n",
    "\n",
    "#### Evaluation Metrics\n",
    "Classifier performance was evaluated using:\n",
    "1. **Accuracy:** Percentage of correctly classified instances.\n",
    "2. **Error Rate:** Percentage of misclassified instances.\n",
    "3. **Confusion Matrix:** Breakdown of true positives, true negatives, false positives, and false negatives for each role.\n",
    "\n",
    "#### Classifier Performance Summary\n",
    "\n",
    "The following table summarizes the performance of each classifier:\n",
    "\n",
    "| **Classifier**             | **Accuracy (%)** | **Error Rate (%)** | **Key Insights**                                     |\n",
    "|----------------------------|------------------|--------------------|-----------------------------------------------------|\n",
    "| Dummy Classifier           | 35.0             | 65.0               | Provided a baseline accuracy for comparison.        |\n",
    "| k-Nearest Neighbors (k=10) | 35.0             | 65.0               | Performed at baseline level, sensitive to feature scaling. |\n",
    "| Naive Bayes Classifier     | 91.0             | 9.0                | Performed significantly better, effectively handling simpler patterns. |\n",
    "| Decision Tree Classifier   | 95.0             | 5.0                | Captured patterns well with some risk of overfitting. |\n",
    "| Random Forest (N=5667, M=10, F=20) | **97.10**       | **2.90**           | Best performer; achieved the highest accuracy and generalization. |\n",
    "\n",
    "##### Evaluation and Comparison\n",
    "- **k-Nearest Neighbors** and **Dummy Classifier** both achieved an accuracy of 35.0% and an error rate of 65.0%, representing baseline performance.\n",
    "- **Naive Bayes** significantly outperformed these models with an accuracy of 91.0% and an error rate of 9.0%, showcasing its ability to handle simpler patterns effectively.\n",
    "- **Decision Trees** improved further, achieving an accuracy of 95.0% with a 5.0% error rate. While effective in identifying patterns, they carried some risk of overfitting.\n",
    "- **Random Forests** delivered the best performance, achieving an accuracy of 97.10% and an error rate of 2.90%. Its ensemble approach effectively reduced overfitting and leveraged diverse features for superior predictions.\n",
    "\n",
    "##### Random Forest Hyperparameter Tuning\n",
    "The Random Forest classifier was optimized by testing various configurations of key hyperparameters:\n",
    "- **N (Number of Instances):** Training with all 5,667 instances resulted in the best performance.  \n",
    "- **M (Number of Features):** Using 10 features provided a balance between accuracy and complexity.  \n",
    "- **F (Max Features per Split):** Setting `F=20` yielded the highest accuracy, demonstrating the importance of allowing a greater number of features per split.\n",
    "\n",
    "The best configuration, with `N=5667`, `M=10`, and `F=20`, resulted in an accuracy of **97.10%**, offering an optimal combination of generalization and computational efficiency.\n",
    "\n",
    "#### Conclusion\n",
    "Random Forests emerged as the best classifier, achieving the highest accuracy (**97.14%**) and lowest error rate (**2.86%**). The ensemble approach effectively reduced overfitting and leveraged diverse features to improve predictive performance. Future work could explore additional hyperparameter optimization and feature engineering to further enhance accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Web App \n",
    "\n",
    " Create a Flask web app with this “best” classifier deployed with an API interface. For the base project (i.e., not bonus), your web app only has to run locally. Part of the bonus is to deploy your web app:\n",
    "\n",
    "1. BONUS (5 pts): Deploy your Flask web app to a free hosting service such as Render (you do not have to use Render, you may use a different service if you wish). In your repo README.md and your project report, include a link to a deployed web app hosting your Flask app.\n",
    "2. BONUS (3 pts): Add a user interface to your Flask web app on the index/homepage. The interface should allow the user to enter in attribute values for an unseen instance via a form, press a “Predict” button, and see the prediction for the instance. See the completed Flask-App-Demo repo on Github for a template of how to do this with Flask and a POST request (I will post this after we cover it in class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "#### Dataset Summary\n",
    "The dataset used in this project comprised detailed attributes of soccer players, such as physical characteristics, skill ratings, and positions. The primary classification task was to categorize players into one of four roles: **Forward**, **Midfielder**, **Defender**, or **Goalkeeper**. One inherent challenge in the dataset was the imbalance in the number of players for each role, which required careful downsampling to ensure balanced training data for the classifiers. Additionally, the presence of correlated features posed challenges for some classifiers, such as Naive Bayes, which assumes feature independence.\n",
    "\n",
    "#### Classification Approach\n",
    "The project employed multiple classification techniques, including:\n",
    "- **k-Nearest Neighbors (k-NN)**\n",
    "- **Naive Bayes**\n",
    "- **Decision Tree**\n",
    "- **Random Forests**\n",
    "- **Dummy Classifier (Baseline)**\n",
    "\n",
    "The features were carefully selected, and the dataset was preprocessed to discretize player positions into four distinct roles. Hyperparameter tuning for the Random Forest classifier allowed for the exploration of different combinations of parameters, such as the number of instances (`N`), features (`M`), and maximum features per split (`F`).\n",
    "\n",
    "#### Classifier Performance\n",
    "Among the classifiers, **Random Forests** achieved the best performance with an accuracy of **97.14%** and an error rate of **2.86%**. This success can be attributed to its ensemble approach, which combines multiple decision trees and reduces overfitting. The other classifiers, such as k-NN and Decision Tree, performed reasonably well but fell short of the Random Forest's performance.\n",
    "\n",
    "#### Improvements and Future Work\n",
    "While the Random Forest classifier achieved excellent results, several areas for improvement were identified:\n",
    "1. **Feature Engineering:** Further exploration of feature combinations, transformations, or dimensionality reduction techniques (e.g., PCA) could improve performance.\n",
    "2. **Hyperparameter Optimization:** A more exhaustive search of hyperparameters, including higher values of `F`, could potentially enhance accuracy.\n",
    "3. **Dataset Expansion:** Increasing the size and diversity of the dataset could improve generalization and performance on unseen data.\n",
    "4. **Ensemble Methods:** Exploring other ensemble methods, such as Gradient Boosting or AdaBoost, could provide additional insights and improvements.\n",
    "\n",
    "In conclusion, this project demonstrated the effectiveness of Random Forests for classifying soccer players' roles, highlighting its robustness and accuracy compared to other classifiers. With further refinement and dataset expansion, the performance could be enhanced even further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acknowledgments: \n",
    "The dataset used in this project was sourced from [Kaggle](https://www.kaggle.com/datasets/maso0dahmed/football-players-data). We extend our gratitude to *Masood Ahmed* and *Talha Turab* for creating and sharing this dataset.\n",
    "\n",
    "Additionally, we utilized OpenAI's ChatGPT to assist with this project. ChatGPT was used to provide guidance and generate code for building visualizations and improving other aspects of the implementation. Its contributions were helpful and streamliend the development process."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
